###This pipeline is adapted from Groves Dixon's ortholog pipeline found in this github repository:
https://github.com/grovesdixon/symbiodinium_orthologs

### Helpful info from Groves:

If you're starting with transcriptomes yes, I would recommend starting at prep data. I'd also separate the host sequences from the symbiont before-hand. The "re-isogrouping" step may be unnecessary since you probably know the level they were clustered at. If you only need orthologs for a pair of species, some steps in that pipeline may be a little overkill (eg those under 'FILTER PARALOGS'). One issue I anticipate is getting a bunch of really large orthologous groups (with lots of sequences from each species), which may be tricky to deal with in terms of comparing expression results. If that happens I can probably alter the paraPrune.py script to work with pairs. 

For annotating with GO terms, I think that pulling them from Uniprot as in the pipeline should work, but now I'm sold on eggNOG mapper (http://eggnogdb.embl.de/#/app/emapper), which gives you not just GO terms, but KEGG and COG and a general name for the gene as well. It takes a bit, but super easy, just upload the protein sequences from Transdecoder.

For your purposes, I think the main purpose of the prep data section to get the species names appended to each sequence definition in the fasta files (this is all that replace_deflines.py does). This helps keep them straight once you're dealing with orhtologs. Then to have your expression results match up with these names, I would concatenate these modified fasta files to the symbiont sequences for mapping.

#IMPORTANT: I will need to re-map once I have created these re-named references so that the sequences match up later on down the line

### Helpful info on parallelizing a job:
#How Dan does it:
>mapping
for file in *.trim
do echo "bowtie2 -x astrangia.fasta -U $file --local -p 4 -S ${file/.trim/}.sam --no-hd --no-sq --no-unal -k 5" >> mapping
done

#creates mapping file, instead of adding info to the top of the file and using qsub, you run the below command...

/projectnb/coral/MPCC_2018/scc6_qsub_launcher.py -N bowtie_mapping -P coral -M wuitchik@bu.edu -j y -h_rt 24:00:00 -jobsfile mapping

#then submit
qsub bowtie_mapping_array.qsub


### Now starting from the beginning of Groves' pipeline

#get scripts from Groves' github and copy into working directory:
[haich@scc1 orthologs]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs

[haich@scc1 orthologs]$ svn checkout https://github.com/grovesdixon/convergent_evo_coral/trunk/scripts_for_pipeline

#---------------------------------------Prep Data
## Install/obtain needed programs:

## Info on FastOrtho from SCC folks:
FastOrtho is now available as a module.
 
module load blast+/2.7.1
module load fastortho/2019_git80c4fa1
 
If needed it can also run with a legacy blast
 
module load blast/2.2.26
module load fastortho/2019_git80c4fa1
 
when the fastortho module is loaded, a convenience variable $SCC_FASTORTHO_EXAMPLES will be available, it points to an examples directory in which is an example running fastortho directly with qsub, and a copy of the using_FastOrtho github you originally linked.
 
ls $SCC_FASTORTHO_EXAMPLES
cd $SCC_FASTORTHO_EXAMPLES
 

## Get swissprot
[haich@scc-pi3 swissprot]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/swissprot

[haich@scc-pi3 swissprot]$ wget 'ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot*sprot.fasta.gz'

[haich@scc-pi3 swissprot]$ module load blast+/2.7.1
[haich@scc-pi3 swissprot]$ makeblastdb -in uniprot_sprot.fasta -input_type fasta -dbtype prot


## Get transdecoder
#Download v5.5.0 from: https://github.com/TransDecoder/TransDecoder/releases
#scp'd to ortholog/ folder on scc
#re-named to 'TransDecoder'
[haich@scc-pi3 orthologs]$ mv TransDecoder-TransDecoder-v5.5.0 TransDecoder


## hmmerscan is already intalled on scc, just module load it:
[haich@scc1 orthologs]$ module load hmmer/3.2.1

#Good tips, to check where all of the files for a module are installed, use: 
module show
[haich@scc1 bin]$ pwd
/share/pkg.7/hmmer/3.2.1/install/bin

# Lots of good info here: http://www.bu.edu/tech/support/research/software-and-programming/software-and-applications/modules/


##Starting with Oculina individual host and symbiont reference transcriptomes
##Note that this pipeline has been updated to include the most recent version of Hanny's Oculina arbuscula and Breviolum psygmophilum transcriptomes starting May 12, 2020


###RENAMING SEQS
[haich@scc-pi3 orthologs]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs

[haich@scc1 orthologs]$ module load python3/3.6.9

#the below command creates a file called 'nameReplacement', definitely more necessary when you have many files but still cool and useful here 
[haich@scc1 orthologs]$ for file in *.fasta
> do echo "replace_deflines.py -fa $file -prefix ${file/.fa/} -o ${file/.fa/}.fasta -clean yes" >> nameReplacement
> done

#I altered the nameReplacement file so I could submit it with a qsub command:
#This script adds the characters after -prefix to the beginning of each contig name in the fasta file you give it with the -fa flag

#!/bin/bash
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N nameReplacement # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

module load python3
python3 replace_deflines.py -fa B_psygmophilum_transcriptome.fasta -prefix BPSG -o B_psygmophilum_transcriptomesta.fasta -clean yes
python3 replace_deflines.py -fa O_arbuscula_transcriptome.fasta -prefix OARB -o O_arbuscula_transcriptomesta.fasta -clean yes

[haich@scc1 orthologs]$ qsub nameReplacement 

##NOTE the names that you replace with here are important. Avoid using underscores in the name, and make them short and unique. Several steps downstream will give you trouble if you replace with the wrong name here. 

##Also outputs a .tsv file for each transcriptome that lists the new contig name next to the original contig name. 


###RE-ISOGROUPING
##run cd-hit for transcriptomes to re-cluster based on the given parameters
#Groves did this step because it wasn't clear how (if at all) transcripts from each transcriptome were grouped into isogroups
#to make sure this is standardized across them, redo it 
#This was done following the Matz lab annotation pipeline from: https://github.com/z0on/annotatingTranscriptomes/blob/master/annotating%20trascriptome.txt:
		# if you have no assembler-derived isogroups, use cd-hit-est to cluster contigs.
		# to look for 98% or better matches between contigs taking 30% of length of either longer or shorter sequence:
		cd-hit-est -i transcriptome.fasta -o transcriptome_clust.fasta -c 0.98 -G 0 -aL 0.3 -aS 0.3
		# adding cluster designations to fasta headers, creating seq2iso table:
		isogroup_namer.pl transcriptome.fasta transcriptome_clust.fasta.clstr 


#more info on cd-hit-est here: https://github.com/weizhongli/cdhit/wiki/3.-User's-Guide#CDHITEST

[haich@scc1 orthologs]$ module load blast
[haich@scc1 orthologs]$ module load cdhit/4.6.8

#create a file called clust that copies the cdhit command for all fasta files in the current working directory
[haich@scc-pi3 orthologs]$ >clust;for file in *.fasta; do echo "cd-hit-est -i $file -o ${file/.fasta/_clust.fa} -c 0.98 -G 0 -aL 0.3 -aS 0.3" >> clust; done

[haich@scc-pi3 orthologs]$ cat clust 
#!/bin/bash -l
#$ -cwd # start job in submission directory
#$ -N clust # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

module load blast
module load cdhit/4.6.8

cd-hit-est -i B_psygmophilum_transcriptomesta.fasta -o B_psygmophilum_transcriptomesta_clust.fa -c 0.98 -G 0 -aL 0.3 -aS 0.3
cd-hit-est -i O_arbuscula_transcriptomesta.fasta -o O_arbuscula_transcriptomesta_clust.fa -c 0.98 -G 0 -aL 0.3 -aS 0.3


[haich@scc-pi3 orthologs]$ qsub clust
#took about 8 minutes to run

##now output only the longest contig from each cluster created from cdhit
#Create job file getLongest:
[haich@scc1 orthologs]$ for file in *.fasta; do echo "longest_isotig.py -i $file -cdh ${file/.fasta/}_clust.fa.clstr -o ${file/.fasta/_longest.fa} > ${file/.fasta/}_getLongest.log" >> getLongest; done

[haich@scc1 orthologs]$ cat getLongest 
#!/bin/bash -l
#$ -cwd # start job in submission directory
#$ -N getLongest # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

module load python3

longest_isotig.py -i B_psygmophilum_transcriptomesta.fasta -cdh B_psygmophilum_transcriptomesta_clust.fa.clstr -o B_psygmophilum_transcriptomesta_longest.fa > B_psygmophilum_transcriptomesta_getLongest.log
longest_isotig.py -i O_arbuscula_transcriptomesta.fasta -cdh O_arbuscula_transcriptomesta_clust.fa.clstr -o O_arbuscula_transcriptomesta_longest.fa > O_arbuscula_transcriptomesta_getLongest.log

#takes about 2 minutes to run
#When the getLongest step works it looks like this:
[haich@scc-pi3 orthologs]$ head Brevolium_psyg_transcriptomesta_getLongest.log

Reading in cd-hit data B_psygmophilum_transcriptomesta_clust.fa.clstr

Found 31428 clusters in cd-hit input file
Found 31428 longest sequences
Recorded 31428 longest sequences in the dictionary (should be equal)
Will not write out 542 shorter isotigs
542 + 31428 = 31970 total sequences

[haich@scc-pi3 orthologs]$ head Oculina_arbuscula_transcriptomesta_getLongest.log

Reading in cd-hit data O_arbuscula_transcriptomesta_clust.fa.clstr

Found 47431 clusters in cd-hit input file
Found 47431 longest sequences
Recorded 47431 longest sequences in the dictionary (should be equal)
Will not write out 10039 shorter isotigs
10039 + 47431 = 57470 total sequences

#Note, this is ~2,000 fewer host contigs and ~80 more symbiont contigs than when I did this with the original transcriptome. 

#keep and combine the log output files from the getLongest qsub command (getLongest.log)
[haich@scc-pi3 orthologs]$ cat *getLongest.log > all_getLongest.log

#check that you have all the longest only files (again, overkill for me with only two transcriptomes but still good sanity check)
[haich@scc-pi3 orthologs]$ ls *longest.fa | wc -l
2

#The _clust.fa output files are in:
[haich@scc-pi3 orthologs]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs

#number of contigs in original transcriptomes:
[haich@scc1 original_refs]$ grep '>' Oculina_arbuscula_transcriptome.fasta | wc -l 
93918
[haich@scc1 original_refs]$ grep '>' Brevolium_psyg_transcriptome.fasta | wc -l
35894

#number of contigs in new transcriptomes:
[haich@scc1 orthologs]$ grep '>' O_arbuscula_transcriptome.fasta | wc -l
57470
[haich@scc1 orthologs]$ grep '>' B_psygmophilum_transcriptome.fasta | wc -l
31970

#number of contigs after clustering (original ref):
[haich@scc1 orthologs]$ grep '>' Oculina_arbuscula_transcriptomesta_clust.fa | wc -l
48686
[haich@scc1 orthologs]$ grep '>' Brevolium_psyg_transcriptomesta_clust.fa | wc -l
31510

#number of contigs after clustering (new ref):
[haich@scc1 orthologs]$ grep '>' O_arbuscula_transcriptomesta.fasta | wc -l
57470
[haich@scc1 orthologs]$ grep '>' B_psygmophilum_transcriptomesta.fasta | wc -l
31970

#these numbers aren't different for the new transcriptome because Hanny re-clustered these files with cdhit to create the new files. So there was nothing left to collapse. 


###### mapping to the concatenated _clust.fa files to get ortholog counts:

[haich@scc1 ortho_mapping]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/ortho_mapping/ortho_ref

[haich@scc1 ortho_ref]$ module load bowtie2
[haich@scc1 ortho_ref]$ cat O_arbuscula_transcriptomesta_clust.fa B_psygmophilum_transcriptomesta_clust.fa > Oculina_new_clustered_transcriptome.fasta

#this is the new reference that Hanny clustered with cdhit
[haich@scc1 ortho_ref]$ bowtie2-build Oculina_new_clustered_transcriptome.fasta Oculina_new_clustered_transcriptome.fasta 

#make mapping file and submit
[haich@scc1 ortho_mapping]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/ortho_mapping

[haich@scc1 ortho_mapping]$ tagseq_bowtie2map.pl "trim$" ortho_ref/Oculina_combined_clustered_transcriptome.fasta > ortho_maps

[haich@scc1 ortho_mapping]$ tagseq_bowtie2map.pl "trim.keepdups$" ortho_ref/Oculina_new_clustered_transcriptome.fasta > ortho_maps_keepdups

#re-made seq2iso.tab file for the orthologs, it is just a two column, tab-separated file that is the name of the ortholog twice. We don't need a typical seq2iso.tab file at this step because we have mapped to the collapsed reference with the longest contigs only. So we essentially lose the variation of the different isogroups. Groves' assumption is that  if isogroup members really are the same gene, any reads that map to the shorter should also map to longer contig, so he'd rather just be done with the shorter contigs before ortholog calling and mapping

[haich@scc1 ortho_mapping]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/ortho_mapping

[haich@scc1 ortho_mapping]$ samcount_launch_bt2.pl '\.sam' ortho_ref/Oculina_combined_clustered_transcriptome_seq2iso.tab > samcount

[haich@scc1 ortho_mapping]$ qsub samcount
[haich@scc1 ortho_mapping]$ expression_compiler.pl *.sam.counts > Oculina_ortholog_counts.txt



#---------------------------------------Getting Protein SEQS
#now working in folder called 'reisogrouped'
[haich@scc1 reisogrouped]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped

#working with the new reference in /reisogrouped_newref/
[haich@scc1 reisogrouped_newref]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref

[haich@scc1 reisogrouped_newref]$ ls *.fa
B_psygmophilum_transcriptomesta_longest.fa  O_arbuscula_transcriptomesta_longest.fa

#----- TRANSDECODER -----#
## STEP 1: GET LONGEST OPEN READING FRAMES (ORFs)

cat tdec1 
#!/bin/bash
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N tdec1_m50 # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be


/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/TransDecoder/TransDecoder.LongOrfs -t Brevolium_psyg_transcriptomesta_longest.fa -m 50
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/TransDecoder/TransDecoder.LongOrfs -t Oculina_arbuscula_transcriptomesta_longest.fa -m 50


#The -m flag for TransDecoder.LongOrfs means it identifies ORFs that are at least 100 amino acids long. 
#Creates an output directory for each transcriptome, for example Brevolium_psyg_transcriptomesta_longest.fa.transdecoder_dir/
#The directory contains the longest_orfs.pep file which is a protein file
#takes about 10 minutes to run


###Groves thinks that we might be able to get a few more orthologs if we change the -m flag to 50 (from 100) at the tdec1 step. This lets in shorter sequences and might boost the final number of Orthologs. Groves set it to 100 originally to get only really good sequences for evolutionary analysis (his paper with Carly), but he used -m 50 at tdec1 in a recent analysis getting Clade C and D orthologs - he is also planning to use this newer analysis for comparative gene expression

#so, after going through all steps for -m 100 with the original reference, made a sub-folder of /reisogrouped/ and started over with tdec1 changes. all subsequent steps were ran on both versions of tdec1
[haich@scc1 redo_tdec1_m50]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped/redo_tdec1_m50

#new reference, still using -m 50
[haich@scc1 reisogrouped_newref]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref

[haich@scc1 reisogrouped_newref]$ cat tdec1
#!/bin/bash
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N tdec1_m50 # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be


/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/TransDecoder/TransDecoder.LongOrfs -t B_psygmophilum_transcriptomesta_longest.fa -m 50
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/TransDecoder/TransDecoder.LongOrfs -t O_arbuscula_transcriptomesta_longest.fa -m 50


## STEP 2: BLAST TO SWISSPROT
#blasting longest open reading frames (.pep files) using blastp (protein blast) to annotate
#make sure to submit blast job with this special command to request more threads. Job was killed the first time because it was using too many cpu's

[haich@scc1 reisogrouped]$ qsub -pe omp 10 spBlast    


[haich@scc1 reisogrouped_newref]$ cat spBlast 
#!/bin/bash -l
#$ -cwd # start job in submission directory
#$ -N spBlast # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

module load blast+/2.7.1

blastp -query B_psygmophilum_transcriptomesta_longest.fa.transdecoder_dir/longest_orfs.pep -db /projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/swissprot/uniprot_sprot.fasta -max_target_seqs 1 -outfmt 6 -evalue 1e-5 -num_threads 8 > B_psygmophilum_transcriptomesta_longest.fa_blastSP.out
blastp -query O_arbuscula_transcriptomesta_longest.fa.transdecoder_dir/longest_orfs.pep -db /projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/swissprot/uniprot_sprot.fasta -max_target_seqs 1 -outfmt 6 -evalue 1e-5 -num_threads 8 > O_arbuscula_transcriptomesta_longest.fa_blastSP.out



#blastp is a protein blast, so here you are blasting the longest_orfs.pep file for each transcriptome against the uniprot_sprot.fasta database 
# -outfmt 6 is a tabular output format



## STEP 3: HMM SCANS (can be run simultaneously with swissprot blast/step 2)
#hmmscan is part of the module hmmer/3.2.1, which lets you scan a sequence against a profile database to parse the sequence into its component domains
 
#did not find it necessary to do the step of first making pfam databases for each species/transcriptome, so skipped that step of Groves' original pipeline

#Lots of good info on this step here: https://github.com/TransDecoder/TransDecoder/wiki
#essentially, identifying open reading frames (ORFs) with homology to known proteins via pfam search
 
#First, download Pfam-A.hmm file which we will need to do the Pfam search (using hmmscan command), which searches the peptides in the longest open reading frames .pep files for protein domains
#Downloaded Pfam-A.hmm from the link under the Pfam Search section at https://github.com/TransDecoder/TransDecoder/wiki

#Prep the Pfam-A.hmm database with the hmmpress command
[haich@scc1 reisogrouped]$ hmmpress Pfam-A.hmm 

#Move all Pfam-A files into their own Pfam directory (/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped
/Pfam)

#Now create a job file called scanHmm to run hmmscan
>scanHmm
for file in *.fa
do PERSONAL_DB_DIR=${file/_longest.fa}_pfam
echo "hmmscan --cpu 8 \
--domtblout ./${file/_longest.fa/}.domtblout ${PERSONAL_DB_DIR}/Pfam-A.hmm ${file}.transdecoder_dir/longest_orfs.pep > ${file/.fasta/}_hmmscan.log" >> scanHmm
done


#with new reference, tried running these jobs separately and increased cpu's. 
[haich@scc1 reisogrouped_newref]$ qsub -pe omp 12 scanHmm_host 
[haich@scc1 reisogrouped_newref]$ qsub -pe omp 12 scanHmm_sym 
#Example:
#!/bin/bash -l
#$ -cwd # start job in submission directory
#$ -N scanHmm_sym # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

module load hmmer/3.2.1
hmmscan --cpu 12 --domtblout ./B_psygmophilum_transcriptomesta.domtblout Pfam/Pfam-A.hmm B_psygmophilum_transcriptomesta_longest.fa.transdecoder_dir/longest_orfs.pep > B_psygmophilum_transcriptomesta_longest.fa_hmmscan.log


#Info on flags:
#--domtblout option produces the domain hits table, summarizes the per-domain output, one data line per homologous domain detected in a query sequence for each homologous model.

#Job took ~18hours to run after changing -m to 50 in tdec1

#Weirdly not getting very many hits to proteins, maybe need a new database from here?:
ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release


## STEP 4: PREDICT PROTEINS FROM LONGEST ORFS AND BLAST/hmmscan
#Using transdecoder again

[haich@scc1 reisogrouped]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped

#setup
>tdec2
for file in *longest.fa; do echo "$TRANSDEC/TransDecoder.Predict -t ${file} --retain_pfam_hits ${file/_longest.fa/}.domtblout --retain_blastp_hits ${file}_blastSP.out --single_best_only" >> tdec2
done


#then submit job tdec2:
[haich@scc1 reisogrouped_newref]$ cat tdec2
#!/bin/bash
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N tdec2 # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/TransDecoder/TransDecoder.Predict -t Brevolium_psyg_transcriptomesta_longest.fa --retain_pfam_hits Brevolium_psyg_transcriptomesta.domtblout --retain_blastp_hits Brevolium_psyg_transcriptomesta_longest.fa_blastSP.out --single_best_only
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/TransDecoder/TransDecoder.Predict -t Oculina_arbuscula_transcriptomesta_longest.fa --retain_pfam_hits Oculina_arbuscula_transcriptomesta.domtblout --retain_blastp_hits Oculina_arbuscula_transcriptomesta_longest.fa_blastSP.out --single_best_only

#-t is the fasta file output from cdhit with longest contigs kept, this is the file we are predicting proteins from
#--retain_pfam_hits and --retain_blastp_hits are ensuring that peptides with blast hits (from _blastSP.out) or domain hits (from .domtblout) are retained in the set of reported likely coding regions

#takes ~30 mins to run


#final results of this step are .pep and .cds files, the pep files are saved here:
[haich@scc-pi3 transdecoders_coding_fas]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped/myPepFastas

#rename the .pep files to .pep.fas in order to run TransDecoder:
[haich@scc1 myPepFastas]$ mv Brevolium_psyg_transcriptomesta_longest.fa.transdecoder.pep Brevolium_psyg_transcriptomesta_longest.fa.transdecoder.pep.fas
[haich@scc1 myPepFastas]$ mv Oculina_arbuscula_transcriptomesta_longest.fa.transdecoder.pep Oculina_arbuscula_transcriptomesta_longest.fa.transdecoder.pep.fas

#the other outputs (.cds, .gff3, .bed) from the TransDecoder.Predict step are saved:
[haich@scc1 reisogrouped]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped


#---------------------------------------CALLING ORTHOLOGS
#FILTER THE COMPILED BLAST RESULTS BASED ON LENGTH

#first get the lengths of each sequence from the blast input files

[haich@scc1 myPepFastas]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/myPepFastas

[haich@scc1 myPepFastas]$ module load python3
[haich@scc1 myPepFastas]$ fasta_sequence_characters.py -fa Oculina_arbuscula_transcriptomesta_longest.fa.transdecoder.pep.fas > Oculina_arbuscula_transcriptomesta_lengths.tsv  

[haich@scc1 myPepFastas]$ fasta_sequence_characters.py -fa Brevolium_psyg_transcriptomesta_longest.fa.transdecoder.pep.fas > Brevolium_psyg_transcriptomesta_lengths.tsv  

[haich@scc1 myPepFastas]$ cat *lengths.tsv > all_lengths.txt

#also created a separate all_lengths.txt file in the tdec1 redo folder:
[haich@scc1 myPepFastas]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped/redo_tdec1_m50/myPepFastas

#Groves says the --pmatch_cutoff argument in FastOrtho seems like it should do this, but didn't work for him

??????????????????????????????????????????????????????????????????????????????????????????
[haich@scc1 myPepFastas]$ filter_blast_for_orthos.py -i /projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped/fastortho_run1/run1.out -l all_lengths.txt -c 0.75 -o reducedCompiled.out
1000000 lines processed
2000000 lines processed

2400480 total lines processed
1566622 hit lenghts were less than 75.0% of the subject lengths and were removed
833858 hits were retained
All hits accounted for.

Done.

#also remove the AdigNCBI sequences
#DONT NEED THIS STEP, SPECIFIC TO GROVES

#---------------------------------------RUN FASTORTHO
#was originally running fastortho twice based on Groves' pipeline. This isn't necessary as long as you get the .end file out of the fastortho run. So, only doing this once now and then moving on to filtering paralogs

### PREPARE OPTIONS FILE
#fastOrtho can take a lot of options, so its
#easiest to set this up and feed it into the command
#you can make a working options file in two ways,
#you can edit the template options file manually and use that
#or set the necessary variables in bash then build one with build_options.sh

Working here:
[haich@scc1 fastortho_run1]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/fastortho_run1


[haich@scc1 myPepFastas]$ module load blast+/2.7.1
[haich@scc1 myPepFastas]$ module load fastortho/2019_git80c4fa1

[haich@scc1 reisogrouped_newref]$ export BLASTP=$(which blastp)     #path to blastp
[haich@scc1 reisogrouped_newref]$ export MAKEBLASTDB=$(which makeblastdb)   #path to makeblastdb
[haich@scc1 reisogrouped_newref]$ export MCL=$(which mcl)   #path to MCl executable
[haich@scc1 reisogrouped_newref]$ export FASTORTHO=$(which FastOrtho)    #path to FastOrtho executable
[haich@scc1 reisogrouped_newref]$ export FAAS="/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/myPepFastas"
#path to .pep.fas files
[haich@scc1 reisogrouped_newref]$ export EVALUE="1e-10"    #the evalue cutoff you want to use
[haich@scc1 reisogrouped_newref]$ export NAME="run1"     #name of this FastOrtho run
[haich@scc1 reisogrouped_newref]$ export OPTIONS="option_file.txt"  #desired name of options file


#build options file
build_options.sh $NAME $OPTIONS

#I followed Groves' way, option_file.txt looks like this:
[haich@scc-bb8 reisogrouped]$ cat option_file_run1.txt 
--project_name run1
--working_directory /projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref
--formatdb_path /share/pkg.7/blast+/2.7.1/install/bin/makeblastdb
--blastall_path /share/pkg.7/blast+/2.7.1/install/bin/blastp
--mcl_path /share/pkg.7/fastortho/2016_git80c4fa1/install/bin/mcl
--pv_cutoff 1e-10
--pi_cutoff 0.0
--pmatch_cutoff 0.0
--maximum_weight 316.0
--result_file ./run1.end
--inflation 1.5
--blast_cpus 12
--blast_b 1000
--blast_v 1000
--blast_e 1e-10
--single_genome_fasta /projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/myPepFastas/Brevolium_psyg_transcriptomesta_longest.fa.transdecoder.pep.fas
--single_genome_fasta /projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/myPepFastas/Oculina_arbuscula_transcriptomesta_longest.fa.transdecoder.pep.fas


[haich@scc-bb8 reisogrouped_newref]$ qsub -pe omp 10 runFastOrtho 
[haich@scc-bb8 reisogrouped_newref]$ cat runFastOrtho 
#!/bin/bash -l
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N fastortho1 # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

module load blast+
module load fastortho

/share/pkg.7/fastortho/2016_git80c4fa1/install/bin/FastOrtho --option_file option_file_run1.txt

#job took about 1 hour to run
#Created a folder called fastortho_run1 and moved all output files into it
#run1.faa output is a concatenation of all .pep.tsv files


#---------------------------------------FILTER PARALOGS
#make new directory and copy the .end file from the fast ortho run 1 into it
[haich@scc1 output_seqs1]$ mkdir output_seqs1
[haich@scc1 output_seqs1]$ cd output_seqs1/
[haich@scc1 output_seqs1]$ cp ../fastortho_run1/run1.end .

[haich@scc1 output_seqs1]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/output_seqs1


#make sure the .end file is in the same directory as your .pep.fas files!

[haich@scc1 output_seqs1]$ pwd
[haich@scc1 output_seqs1]$ module load python3

#figure out how many protein groups we have
#the -cut argument here means that an ortholog has to be in two taxa to include it. This is necessary because the orthologous groups output from FastOrtho don't have to include both species. 

#here is the output after re-starting from the tdec1 -m 50 
#got us 1970 total protein groups
[haich@scc1 output_seqs1]$ output_seqs_step1.py -orthos run1.end -prot *.pep.fas -cut 2

Results stored in Orthologs_12_18

Reading in fasta files...
Brevolium_psyg_transcriptomesta_longest.fa.transdecoder.pep.fas...
Oculina_arbuscula_transcriptomesta_longest.fa.transdecoder.pep.fas...

 total orthogroups found in 11939
9969 groups had total taxa below 2 and were skipped
1970 total protein groups written to Orthologs_12_18/protein_sequences

#back to 1951 with the new reference
[haich@scc1 output_seqs1]$ output_seqs_step1.py -orthos run1.end -prot *.pep.fas -cut 2

Results stored in Orthologs_5_15

Reading in fasta files...
B_psygmophilum_transcriptomesta_longest.fa.transdecoder.pep.fas...
O_arbuscula_transcriptomesta_longest.fa.transdecoder.pep.fas...

 total orthogroups found in 11678
9727 groups had total taxa below 2 and were skipped
1951 total protein groups written to Orthologs_5_15/protein_sequences

# ALIGN THEM AND BUILD GENE TREES
cd Orthologs_DATE_/protein_sequences/
#there are 1951 .fasta files in this folder now (same number as the total protein groups)
#or 1970 for the tdec1 -m 50 redo version

[haich@scc1 protein_sequences]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/output_seqs1/Orthologs_5_15/protein_sequences

>doAln
for file in *.fasta
do echo "mafft --maxiterate 1000 --localpair $file > ${file/.fasta/}.aln && FastTree ${file/.fasta/}.aln > ${file/.fasta/}.newick" >> doAln
done

#first few lines of job look like this, be sure to add the module load for mafft and FastTree
#mafft is a multiple sequence alignment program (https://mafft.cbrc.jp/alignment/software/)
#FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences (http://www.microbesonline.org/fasttree/)

[haich@scc1 protein_sequences]$ head -20 doAln
#!/bin/bash -l
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N doAln # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be

module load mafft/7.305
module load fasttree/2.1.11


mafft --maxiterate 1000 --localpair ORTHOMCL106.fasta > ORTHOMCL106.aln && FastTree ORTHOMCL106.aln > ORTHOMCL106.newick
mafft --maxiterate 1000 --localpair ORTHOMCL1083.fasta > ORTHOMCL1083.aln && FastTree ORTHOMCL1083.aln > ORTHOMCL1083.newick
mafft --maxiterate 1000 --localpair ORTHOMCL1091.fasta > ORTHOMCL1091.aln && FastTree ORTHOMCL1091.aln > ORTHOMCL1091.newick
mafft --maxiterate 1000 --localpair ORTHOMCL1096.fasta > ORTHOMCL1096.aln && FastTree ORTHOMCL1096.aln > ORTHOMCL1096.newick
mafft --maxiterate 1000 --localpair ORTHOMCL1102.fasta > ORTHOMCL1102.aln && FastTree ORTHOMCL1102.aln > ORTHOMCL1102.newick
mafft --maxiterate 1000 --localpair ORTHOMCL1105.fasta > ORTHOMCL1105.aln && FastTree ORTHOMCL1105.aln > ORTHOMCL1105.newick
mafft --maxiterate 1000 --localpair ORTHOMCL1107.fasta > ORTHOMCL1107.aln && FastTree ORTHOMCL1107.aln > ORTHOMCL1107.newick
mafft --maxiterate 1000 --localpair ORTHOMCL111.fasta > ORTHOMCL111.aln && FastTree ORTHOMCL111.aln > ORTHOMCL111.newick


#THEN PRUNE TREES TO GET SINGLE-COPY ORTHOLOGS
#only doing this step on the tdec1 output

[haich@scc1 protein_sequences]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped_newref/output_seqs1/Orthologs_5_15/protein_sequences

#updated this to the newest version of paraPrune.py that is on Groves' github (3/12/2020)
[haich@scc1 protein_sequences]$ module load python3
[haich@scc1 protein_sequences]$ paraPrune.py -trees *.newick -l all_lengths.txt -subsets True > paraprune.log

#output is singleCopyOrthos.txt
[haich@scc1 protein_sequences]$ cat paraprune.log
Running paraPrune.py...

Will save subsets of trees at each step.

Reading in trees...
	0 tree files were empty, suggest checking these:
	1951 total trees for analysis

Gathering single copy trees...
	1951 total trees checked for paralogs
	468 trees were single copy
	1483 got paralogs
	Writing out 468 initally single copy trees to subdirectory A_init_single_copy_trees...
	Writing out 1483 initally with paralogs trees to subdirectory A_init_paralog_trees...

Looking for single-species clades...
	4738 single-species clades were collapsed into single sequence

Outputting collapsed contigs. These can be summed back in later in RNAseq analysis.
Total collapsed contigs = 4778
Writing these out to collapsed_contigs.tsv...

Gathering additional single copy trees after collapsing single-species nodes...
	1483 total trees checked for paralogs
	470 trees were single copy
	1013 got paralogs
	938 total single copy orthogroups
	Writing out 470 single-copy post pruning trees to subdirectory B_post_prune_single_copy_trees...
	Writing out 1013 with paralogs post pruning trees to subdirectory B_post_prune_paralog_trees...

Pruning away anemone species and Adig references...
	0 leafs from unimportant group pruned away
	333 repeated species within resulting polytomies were pruned

Gathering additional single copy trees after pruning anemones and Adig reference...
	1013 total trees checked for paralogs
	333 trees were single copy
	680 got paralogs
	1271 total single copy orthogroups
	Writing out 333 single-copy post pruning unimportant trees to subdirectory C_post_prune_single_copy_trees...
	Writing out 680 with paralogs post pruning trees to subdirectory C_post_prune_paralog_trees...

Pruning away ANY repeated species branches in trees with median number of appearances == 1...

Gathering additional single copy trees after pruning ANY remainging duplicates if they are rare...
	680 total trees checked for paralogs
	0 trees were single copy
	680 got paralogs
	1271 total single copy orthogroups
	Writing out 0 single-copy post pruning ANY trees to subdirectory D_post_prune_single_copy_trees...
	Writing out 680 with paralogs post pruning ANY trees to subdirectory D_post_prune_paralog_trees...

Pulling out any perfect subtrees from remaining trees with paralogs
	Checking through 680 trees with paralogs
	680 trees had at least one perfect subtree that was made into its own ortholog
	691 total perfect nodes were found
	0 total trees were separated completely into perfect subtrees
	691 total perfect subtrees found among 680 full trees
	147 trees still had at least 3 terminal branches left after pulling perfect subtrees
	-------------
	final check for single copy trees
	147 total trees checked for paralogs
	0 trees were single copy
	147 got paralogs
	1962 total single copy orthogroups
	Writing out 0 single-copy post pull perfect trees to subdirectory E_post_perfect_node_single_copy_trees...
	Writing out 147 with paralogs post pruning ANY trees to subdirectory F_paralog_trees...

Single copy orthos saved as singleCopyOrthos.txt...

Done.

Time took to run: 17.50826334953308


#Info on how to interpret this log file:
#The log describes the number of additional 'single-copy' orthologs found with each iteration. So from this one, we first get 468 single copy orthologs straight from FastOrtho, without any pruning or anything. Then we collapse the single-species clades (pruning paralogs). After pruning paralogs, we look through trees again we find 470 more single copy orthologs. These plus the original 468 gives us a total of 938. It keeps going like that through each step, trying to pull more and more single-copy orthogs. So your final total should be 1962 orthologs, which should be equal to the number of unique strings in the first column of singleCopyOrthos.txt.


#1962 single copy orthologs with new reference
#------- IN CASE YOU WANT TO PULL JUST INITIAL SINGLE COPY ORTHOLOGS

[haich@scc1 protein_sequences]$ cd A_init_single_copy_trees/
[haich@scc1 A_init_single_copy_trees]$ ls *_paraPruned.newick | sed 's/_paraPruned.newick//' > init_single_copy_orthoGroups.txt

#### Skip the Build Species Tree section of Groves' pipeline, this was specifically for Groves' paper with Carly where he had many species

#---------------------------------------ANNOTATING PROTEIN FASTA FILES

#Using the .pep.fas files from the tdec1 re-do
2020-01-14 17:11 /Ortholog_Stuff/--% scp haich@scc1.bu.edu:/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/reisogrouped/redo_tdec1_m50/myPepFastas/*.pep.fas .
Password: 
Brevolium_psyg_transcriptomesta_longest.fa.tr 100%   20MB   9.4MB/s   00:02    
Oculina_arbuscula_transcriptomesta_longest.fa 100%   29MB  10.5MB/s   00:02    

#submit to the eggnog website: http://eggnog-mapper.embl.de/
#used all standard settings, didn't change anything.


A note on double checking things from Groves:
#Since all this is totally new, it's worth checking is how well the annotations that EggNog gives you match the pairs assigned in the singleCopyOrthos.txt table. They should theoretically be always the same, so if pairs in singleCopyOrthos.txt are often assigned to different genes in EggNOG, we may need to change methods.


#---------------------------------------MAPPING TO RE-NAMED REFERENCE


[haich@scc1 ref_for_orthos]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/ref/ref_for_orthos

[haich@scc1 ref_for_orthos]$ cp /projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/*.fasta .

#concatenate re-named host and symbiont references
[haich@scc1 ref_for_orthos]$ cat Brevolium_psyg_transcriptomesta.fasta Oculina_arbuscula_transcriptomesta.fasta > Oarb_Bpsg_combined_ortho.fasta

#make them into a bowtie database
[haich@scc1 ref_for_orthos]$ bowtie2-build Oarb_Bpsg_combined_ortho.fasta Oarb_Bpsg_combined_ortho.fasta

#now for mapping...
[haich@scc1 ortho_mapping]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/ortho_mapping

[haich@scc1 ortho_mapping]$ tagseq_bowtie2map.pl "trim$" /projectnb/davies-hb/hannah/MPCC_2018/Oculina/ref/ref_for_orthos/Oarb_Bpsg_combined_ortho.fasta > ortho_maps

[haich@scc1 ortho_mapping]$ head -20 ortho_maps
#!/bin/bash
#$ -cwd # start job in submission directory
#$ -N ortho_maps # job name, anything you want
#$ -l h_rt=24:00:00 #maximum run time
#$ -M hannahaichelman@gmail.com #your email
#$ -m be


module load bowtie2

bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OA6_S20_R1_001.fastq.trim -S Oculina_OA6_S20_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OR8_S21_R1_001.fastq.trim -S Oculina_OR8_S21_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OL7_S2_R1_001.fastq.trim -S Oculina_OL7_S2_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OC4_S29_R1_001.fastq.trim -S Oculina_OC4_S29_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OI1_S15_R1_001.fastq.trim -S Oculina_OI1_S15_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OQ1_S41_R1_001.fastq.trim -S Oculina_OQ1_S41_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OR7_S39_R1_001.fastq.trim -S Oculina_OR7_S39_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OE3_S30_R1_001.fastq.trim -S Oculina_OE3_S30_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OD4_S40_R1_001.fastq.trim -S Oculina_OD4_S40_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5
bowtie2 --local -x ortho_ref/Oculina_combined_clustered_transcriptome.fasta -U Oculina_OE11_S3_R1_001.fastq.trim -S Oculina_OE11_S3_R1_001.fastq.trim.sam --no-hd --no-sq --no-unal -k 5


### Trying out some different cleaning of the Oculina files to see what happens with not doing the TagseqClipper.pl script

[haich@scc1 clean_noclipper]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/clean_noclipper

[haich@scc1 clean_noclipper]$ module load fastx-toolkit

[haich@scc1 Oculina]$ fastx_clipper -a AAAAAAAA -l 20 -Q33 -i Oculina_OL6_S16_R1_001.fastq -o NoClip1_Oculina_OL6_S16_R1_001.fastq.trim

[haich@scc1 clean_noclipper]$ fastx_clipper -a AGATCGGAAG -l 20 -Q33 -i NoClip1_Oculina_OL6_S16_R1_001.fastq.trim -o NoClip2_Oculina_OL6_S16_R1_001.fastq.trim

[haich@scc1 clean_noclipper]$ fastq_quality_filter -Q33 -q 20 -p 90 -i NoClip2_Oculina_OL6_S16_R1_001.fastq.trim -o NoClip3_Oculina_OL6_S16_R1_001.fastq.trim

#summary of how many reads are in these files at different stages:

[haich@scc1 clean_noclipper]$ grep '@' Oculina_OL6_S16_R1_001.fastq | wc -l
4369379
[haich@scc1 clean_noclipper]$ grep '@' Oculina_OL6_S16_R1_001.fastq.trim | wc -l
1204080
[haich@scc1 clean_noclipper]$ grep '@' NoClip1_Oculina_OL6_S16_R1_001.fastq.trim | wc -l
3726223
[haich@scc1 clean_noclipper]$ grep '@' NoClip2_Oculina_OL6_S16_R1_001.fastq.trim | wc -l
3430643
[haich@scc1 clean_noclipper]$ grep '@' NoClip3_Oculina_OL6_S16_R1_001.fastq.trim | wc -l


#also tried with cutadapt instead of fastx-toolkit
[haich@scc1 clean_noclipper]$ module load cutadapt

[haich@scc1 clean_noclipper]$ cutadapt -a AAAAAAAA -o cutadapt1.fastq Oculina_OL6_S16_R1_001.fastq

[haich@scc1 clean_noclipper]$ cutadapt -a AGATCGGAAG -q 15 -m 25 -o cutadapt2.fastq cutadapt1.fastq

Total reads processed:               3,668,299
Reads with adapters:                   100,321 (2.7%)
Reads that were too short:             209,852 (5.7%)
Reads written (passing filters):     3,458,447 (94.3%)


##From Misha's TagSeq github:
# creating cleaning process commands for all files:
>clean
for F in *.fq; do
echo "tagseq_clipper.pl $F | cutadapt - -a AAAAAAAA -a AGATCGG -q 15 -m 25 -o ${F/.fq/}.trim" >>clean;
done

-a tells it the sequence of an adapter ligated to the 3' end of the first read. The adapter and subsequent bases are trimmed

-q is the quality cutoff, trim low-quality bases before adapter removal

-m minimum length

-o output file name

##Trying to just trim off the first 7 bases (4 degens and GGG) using BBMap
#Following Nicola's GitHub: https://github.com/Nicfall/moorea_holobiont/blob/master/mr_16S/mr16s.R

#Downloaded BBMap from here:
https://sourceforge.net/projects/bbmap/

#and moved to the cluster
[haich@scc1 clean_noclipper]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/clean_noclipper

[haich@scc1 clean_noclipper]$ tar -xvzf BBMap_38.79.tar.gz 

#test that it's working
[haich@scc1 clean_noclipper]$ bbmap/stats.sh in=bbmap/resources/phix174_ill.ref.fa.gz

#use forcetrimleft to remove first 7 bases:

[haich@scc1 clean_noclipper]$ bbmap/bbduk.sh in=cutadapt1.fastq ftl=7 out=cutadapt1.fastq.nodegens

Input is being processed as unpaired
Started output streams:	0.019 seconds.
Processing time:   		2.434 seconds.

Input:                  	3668299 reads 		175228578 bases.
FTrimmed:               	3667147 reads (99.97%) 	25583933 bases (14.60%)
Total Removed:          	135022 reads (3.68%) 	25962295 bases (14.82%)
Result:                 	3533277 reads (96.32%) 	149266283 bases (85.18%)

Time:                         	2.455 seconds.
Reads Processed:       3668k 	1493.94k reads/sec
Bases Processed:        175m 	71.36m bases/sec

##weirdly its removing reads too...should't be doing this. 
#also doesn't seem to be because we have reads shorter than 7 bp:

[haich@scc1 clean_noclipper]$ bbmap/bbduk.sh in=Oculina_OL6_S16_R1_001.fastq out=Oculina_OL6_S16_R1_001.fastq.min minlen=7


\Input is being processed as unpaired
Started output streams:	0.033 seconds.
Processing time:   		2.358 seconds.

Input:                  	3668299 reads 		187083249 bases.
Total Removed:          	0 reads (0.00%) 	0 bases (0.00%)
Result:                 	3668299 reads (100.00%) 	187083249 bases (100.00%)

Time:                         	2.392 seconds.
Reads Processed:       3668k 	1533.48k reads/sec
Bases Processed:        187m 	78.21m bases/sec


##### mapping to only single copy ortholog concatenated file

[haich@scc1 justorthos]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/ortho_mapping/ortho_ref/justorthos

#use the program seqtk to subset the fasta files based on the list of single copy orthologs

[haich@scc1 justorthos]$ module load seqtk

[haich@scc1 justorthos]$ seqtk subseq B_psygmophilum_transcriptomesta_clust.fa singleCopyOrthos_suffixremoved_Sym.txt > B_psygmophilum_transcriptomesta_clust_justorthos.fa

[haich@scc1 justorthos]$ seqtk subseq O_arbuscula_transcriptomesta_clust.fa singleCopyOrthos_suffixremoved_Host.txt > O_arbuscula_transcriptomesta_clust_justorthos.fa

#compare file size now
[haich@scc1 ortho_ref]$ wc -l *_clust.fa
   722789 B_psygmophilum_transcriptomesta_clust.fa
  1213440 O_arbuscula_transcriptomesta_clust.fa

[haich@scc1 justorthos]$ wc -l *justorthos.fa
   3924 B_psygmophilum_transcriptomesta_clust_justorthos.fa
   3924 O_arbuscula_transcriptomesta_clust_justorthos.fa

[haich@scc1 ortho_mapping]$ cat B_psygmophilum_transcriptomesta_clust_justorthos.fa O_arbuscula_transcriptomesta_clust_justorthos.fa > Oculina_new_clustered_justorthos.fasta

[haich@scc1 ortho_mapping]$ bowtie2-build Oculina_new_clustered_justorthos.fasta Oculina_new_clustered_justorthos.fasta

[haich@scc1 ortho_mapping]$ tagseq_bowtie2map.pl "trim$" ortho_ref/justorthos/Oculina_new_clustered_justorthos.fasta > ortho_maps_justorthos

[haich@scc1 ortho_mapping]$ qsub ortho_maps_justorthos 

##### Re-did this mapping with the culture .trim files included (June 27 2022):
[haich@scc1 ortho_mapping]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/ortho_mapping

tagseq_bowtie2map.pl "trim$" ortho_ref/justorthos/Oculina_new_clustered_justorthos.fasta > ortho_maps_justorthos

qsub ortho_maps_justorthos

## Generate read counts per ortholog
samcount_launch_bt2.pl '\.sam' ortho_ref/justorthos/Oculina_new_clustered_justorthos_seq2iso.tab > samcount_justorthos

## Compile counts
expression_compiler.pl *.sam.counts > all_ortho_counts.txt

## Moved all of the files for this run (all comparisons, mapping to just single count orthologs) here:
[haich@scc1 maps_justorthos_allsamps]$ pwd
/projectnb/davies-hb/hannah/MPCC_2018/Oculina/orthologs/ortho_mapping/maps_justorthos_allsamps

## From here, have to run 'assemble_ortholog_counts_hea.R' in order to create the counts file that DESeq uses.
